# DeepSeek Local - Context for Claude Code

## About the Project

This project sets up a Docker environment to run AI models (DeepSeek V3, Qwen, Code Llama) locally, eliminating dependency on external APIs and token limitations.

## Architecture

- **Docker Compose**: Orchestrates Ollama containers + Web Interface
- **Ollama**: Runtime for local LLM models
- **Open WebUI**: Web interface for interaction
- **Bash scripts**: Setup and management automation

## File Structure

```
deepseek-local/
├── docker-compose.yml     # Container configuration
├── .env                  # Environment variables and settings
├── scripts/
│   ├── setup.sh         # Initial installation script
│   ├── start.sh         # Daily usage commands
│   └── install-models.sh # Model management
├── README.md            # User documentation
└── CLAUDE.md           # This file (context for Claude Code)
```

## Main Configurations

### Docker Compose (`docker-compose.yml`)
- **ollama**: Main container running on port 11434
- **open-webui**: Web interface on port 3000
- **Persistent volumes**: To store downloaded models
- **Resource limitation**: Configurable via .env
- **Health checks**: To ensure functionality

### Environment Variables (`.env`)
- `OLLAMA_PORT`: API port (default: 11434)
- `WEBUI_PORT`: Web interface port (default: 3000)
- `MEMORY_LIMIT`: RAM limit for container
- `DEFAULT_MODELS`: Automatically installed models

## Automation Scripts

### `setup.sh`
- Checks dependencies (Docker, Docker Compose)
- Starts containers
- Installs default models
- Shows access URLs

### `start.sh`
Main manager with commands:
- `start/stop/restart`: Service control
- `status`: Checks container health
- `models`: Lists installed models
- `install`: Installs new models
- `test`: Tests API with prompt
- `logs`: Views real-time logs

### `install-models.sh`
- Download with visual progress
- Checks if model already exists
- Support for multiple models
- Lists recommended models by size

## Supported Models

### Small (4-8GB RAM)
- `qwen2.5-coder:7b`: Optimized for code
- `deepseek-coder:6.7b`: Programming specialist
- `codellama:7b`: Meta Code Llama

### Medium (15-25GB RAM)
- `deepseek-v3`: Best overall quality (~14B parameters)
- `qwen2.5-coder:32b`: Advanced specialist

### Large (30GB+ RAM)
- `llama3.3:70b`: Maximum quality (quantized)

## API Usage

### Main Endpoint
`POST http://localhost:11434/api/generate`

### Example Payload
```json
{
  "model": "deepseek-v3",
  "prompt": "Create a Node.js function to read CSV",
  "stream": false
}
```

### Response Format
```json
{
  "response": "content generated by the model",
  "done": true
}
```

## Common Use Cases

1. **Unlimited development**: Code completion, refactoring, debugging
2. **Rapid prototyping**: Initial code generation
3. **Code review**: Analysis and improvement suggestions
4. **Documentation**: Automatic docs generation
5. **Code translation**: Between languages (Node.js ↔ Ruby)

## Hardware Requirements

### Minimum
- 16GB RAM (for 7B models)
- 10GB disk space
- Multi-core CPU

### Recommended
- 32GB RAM (for DeepSeek V3)
- 50GB disk space
- Apple M-series or modern x86 CPU

### Ideal
- 64GB+ RAM (multiple large models)
- Fast SSD
- Dedicated GPU (future support)

## Common Troubleshooting

### Container won't start
- Check available RAM
- Check ports in use
- Validate Docker permissions

### Model not responding
- Confirm installation: `./scripts/start.sh models`
- Check logs: `./scripts/start.sh logs`
- Test API: `./scripts/start.sh test`

### Slow performance
- Increase `MEMORY_LIMIT` in .env
- Use smaller models (7B vs 70B)
- Close unnecessary applications

## Development and Contribution

### Code Standards
- Bash scripts with error checking (`set -e`)
- Colored codes for better UX
- Modular and reusable functions
- Inline documentation in scripts

### Possible Extensions
- GPU support (NVIDIA/AMD)
- Multiple simultaneous models
- Custom CLI interface
- VSCode/IDE integration
- API proxy with load balancing

## Security Considerations

- **Local network only**: Don't expose publicly without authentication
- **Firewall**: Configure appropriate rules
- **Updates**: Keep Docker and images updated
- **Backup**: Volumes contain valuable models (GBs)

## Performance Tips

1. **SSD**: Models load faster
2. **RAM**: More RAM = larger models + better cache
3. **CPU**: More cores = faster inference
4. **Swap**: Disable or configure appropriately to avoid slowdowns
